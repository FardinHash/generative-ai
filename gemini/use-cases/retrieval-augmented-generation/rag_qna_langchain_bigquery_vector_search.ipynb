{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f-Mv-2s4nDQ"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOnmZT-BRT9t"
   },
   "source": [
    "# Augment Q&A Generation using LangChain ü¶úÔ∏èüîó and BigQuery Vector Search\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/rag_qna_langchain_bigquery_vector_search.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2F%2Frag_qna_langchain_bigquery_vector_search.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/rag_qna_langchain_bigquery_vector_search.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/rag_qna_langchain_bigquery_vector_search.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKrvoBKm42uq"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Ashley Xu](https://github.com/ashleyxuu) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdjbkYYD42-_"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates implementing a Question Answering (QA) system to show how to improve LLM's response by augmenting LLM's knowledge with external data sources such as documents. The notebooks uses Vertex AI Gemini Pro 1.0 for [Text](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview), [Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), [BigQuery Vector Search](https://python.langchain.com/docs/integrations/vectorstores/google_bigquery_vector_search) and [LangChain ü¶úÔ∏èüîó](https://python.langchain.com/en/latest/).\n",
    "\n",
    "### Context\n",
    "\n",
    "Large Language Models (LLMs) have improved quantitatively and qualitatively. They can learn new abilities without being directly trained on them. However, there are constraints with LLMs - they are unaware of events after training and it is almost impossible to trace the sources to their responses. It is preferred for LLM based systems to cite their sources and be grounded in facts.\n",
    "\n",
    "To solve for the constraints, one of the approaches is to augment the prompt sent to LLM with relevant data retrieved from an external knowledge base through Information Retrieval (IR) mechanism.\n",
    "\n",
    "This approach is called Retrieval Augmented Generation (RAG), also known as Generative QA in the context of the QA task. There are two main components in RAG based architecture: (1) Retriever and (2) Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v3ABuqg5Htc"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Install Vertex AI SDK, other packages and their dependencies\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f0Ui2V95KaM"
   },
   "outputs": [],
   "source": [
    "# Install LangChain and Google Cloud BigQuery\n",
    "!pip install --upgrade --quiet tiktoken langchain langchain_google_vertexai google-cloud-bigquery pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83fJGokg5NQC"
   },
   "source": [
    "#### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ5eRC1y5MeR",
    "outputId": "c4a619dd-049f-4899-8ea2-f66b215d7278"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQS6_TCC5Q2l"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è Before proceeding, please wait for the kernel to finish restarting ‚ö†Ô∏è</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp0iuaXh5vIo"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process.\n",
    "\n",
    "If you are using Vertex AI Workbench, you will not require additional authentication.\n",
    "\n",
    "For more information, you can check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujT9cZE55vrf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fGoe4X5zV7"
   },
   "source": [
    "- If you are running this notebook in a local development environment:\n",
    "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
    "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
    "\n",
    "    ```bash\n",
    "    gcloud auth application-default login\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86lzgs_c82nm"
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ycu-4EEZvbsu",
    "outputId": "8e737937-ea3f-4326-dcb2-22ae0bbd3979"
   },
   "outputs": [],
   "source": [
    "# @title Project { display-mode: \"form\" }\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YdP-6DtwnUu"
   },
   "outputs": [],
   "source": [
    "# @title Region { display-mode: \"form\" }\n",
    "REGION = \"US\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDSM2ZQa8eBs"
   },
   "source": [
    "# Add documents to `BigQueryVectorSearch`\n",
    "\n",
    "This step ingests and parse PDF documents, split them, generate embeddings and add the embeddings to the vector store. The document corpus used as dataset is a collection of owners car manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIML1Ttj5nTk"
   },
   "source": [
    "**Summary steps**\n",
    "- Create text embeddings: LangChain `VertexAIEmbeddings`\n",
    "- Ingest PDF files: LangChain `PyPDFLoader`\n",
    "- Chunk documents: LangChain `TextSplitter`\n",
    "- Create Vector Store: LangChain `BigQueryVectorSearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3hRmLiY2O0w"
   },
   "source": [
    "### Create the VertexAI Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tRYGd_Mv_ix"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_community.vectorstores import BigQueryVectorSearch\n",
    "\n",
    "embedding = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKM_QIU0WPc3"
   },
   "source": [
    "### Ingest PDF file\n",
    "\n",
    "The document is hosted on Cloud Storage bucket (at `gs://github-repo/generative-ai/sample-apps/fixmycar/cymbal-starlight-2024.pdf`) and LangChain provides a convenient document loader [`PyPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf/) to load documents from pdfs.\n",
    "\n",
    "LangChain also provides a convenient document loader [`GCSFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_file) to load documents from a Cloud Storage bucket. The loader uses `Unstructured` package to load files of many types including pdfs, images, html and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17eO5HAx8HcG"
   },
   "source": [
    "Make a Google Cloud Storage bucket in your GCP project to copy the document files into, or directlt copy the file to the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNEClQQjjRcc",
    "outputId": "de918892-4fae-4c5b-cd4d-915f87d11a06"
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET_DOCS = (\n",
    "    \"github-repo/generative-ai/sample-apps/fixmycar\"  # @param {type: \"string\"}\n",
    ")\n",
    "\n",
    "# Copy the file to the current path\n",
    "!gsutil cp gs://$GCS_BUCKET_DOCS/*.pdf ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kYw66EohYza",
    "outputId": "098e5ce1-cc21-4be4-c4c0-f2f1e203e5d6"
   },
   "outputs": [],
   "source": [
    "# Ingest PDF files\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"cymbal-starlight-2024.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# Add document name and source to the metadata\n",
    "for document in documents:\n",
    "    doc_md = document.metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    # derive doc source from Document loader\n",
    "    doc_source_prefix = \"/\".join(GCS_BUCKET_DOCS.split(\"/\")[:3])\n",
    "    doc_source_suffix = \"/\".join(doc_md[\"source\"].split(\"/\")[4:-1])\n",
    "    source = f\"{doc_source_prefix}/{doc_source_suffix}\"\n",
    "    document.metadata = {\"source\": source, \"document_name\": document_name}\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8J7hQkVmkB1"
   },
   "source": [
    "Verify document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn3A13Nfmihf",
    "outputId": "3d56d945-7458-4d03-f6ce-09b9f687bada"
   },
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZA9HDA0mnWW"
   },
   "source": [
    "## Chunk documents - TextSplitter\n",
    "\n",
    "Split the documents to smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2QuilhMmxgA",
    "outputId": "1acae5ed-82ea-4d30-85b3-804976fbcb54"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fv91BaKCmxP0",
    "outputId": "fce7f8af-94e7-44d6-e554-84a682dc41d4"
   },
   "outputs": [],
   "source": [
    "doc_splits[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QC-w_t3m9Fe"
   },
   "source": [
    "## Configure `BigQueryVectorSearch` as Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5x-_Epnn-3a"
   },
   "outputs": [],
   "source": [
    "DATASET = \"[my-dataset]\"  # @param {type: \"string\"}\n",
    "TABLE = \"[my-table]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eenn_NAsnF5D",
    "outputId": "b8441c7e-23b3-482d-bd3b-275140f4f405"
   },
   "outputs": [],
   "source": [
    "bq_vector_cars_manual = BigQueryVectorSearch(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    location=REGION,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5fEJXjdoe4x",
    "outputId": "381081da-7f52-4f76-ac5a-2d550850927b"
   },
   "outputs": [],
   "source": [
    "bq_vector_cars_manual.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0RTlZLY_LFP"
   },
   "source": [
    "Verify the BigQueryVectorSearch with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAPITufi-Qca",
    "outputId": "558c53e2-eaae-44f3-8b47-1c8d095eadaf"
   },
   "outputs": [],
   "source": [
    "bq_vector_cars_manual.similarity_search(\n",
    "    \"What should I do when call the emergency roadside assistance?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC9TcmKSF_ni"
   },
   "source": [
    "# Retrieval based Question/Answering Chain\n",
    "\n",
    "We will demonstrate using four LangChain retrieval Q&A chains:\n",
    "\n",
    "- `RetrievalQA`\n",
    "- `RetrievalQAWithSourcesChain`\n",
    "- `ConversationalRetrievalChain`\n",
    "- Advanced: customized Q&A prompt and format\n",
    "\n",
    "We begin by initializing a Vertex AI LLM and a LangChain `retriever` to fetch documents from our BigQuery Vector Search.\n",
    "\n",
    "For Q&A chains our retriever is passed directly to the chain and can be used automatically without any further configuration.\n",
    "\n",
    "Behind the scenes, first the search query is passed to the retriever which runs a search and returns relevant document chunks. These chunks are then passed to the prompt used by the LLM to be used as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQz2A7uzGH7F"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = VertexAI(model_name=\"gemini-pro\")\n",
    "\n",
    "retriever = bq_vector_cars_manual.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKA_D9JGGuTF"
   },
   "source": [
    "### [`RetrievalQA` chain](https://python.langchain.com/docs/modules/chains/popular/vector_db_qa)\n",
    "\n",
    "This is the simplest document Q&A chain offered by LangChain.\n",
    "\n",
    "There are several different chain types available, listed [here](https://docs.langchain.com/docs/components/chains/index_related_chains).\n",
    "\n",
    "- In these examples we use the `stuff` type, which simply inserts all of the document chunks into the prompt.\n",
    "- This has the advantage of only making a single LLM call, which is faster and more cost efficient\n",
    "- However, if we have a large number of search results we run the risk of exceeding the token limit in our prompt, or truncating useful information.\n",
    "- Other chain types such as `map_reduce` and `refine` use an iterative process which makes multiple LLM calls, taking individual document chunks at a time and refining the answer iteratively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uqNH32zGo-E",
    "outputId": "06420428-781b-45cf-d9d5-22cf59fea2c8"
   },
   "outputs": [],
   "source": [
    "search_query = \"How many miles can I drive the 2024 Google Starlight until I need an oil change?\"  # @param {type:\"string\"}\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "retrieval_qa.invoke(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTpq842Vtrar"
   },
   "source": [
    "#### Inspecting the process\n",
    "\n",
    "If we add `return_source_documents=True` we can inspect the document chunks that were returned by the retriever.\n",
    "\n",
    "This is helpful for debugging, as these chunks may not always be relevant to the answer, or their relevance might not be obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M76KDO2UtyBd",
    "outputId": "a39be8ed-77a4-4e11-9ea9-27321c5a85f0"
   },
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "results = retrieval_qa.invoke(search_query)\n",
    "\n",
    "print(\"*\" * 79)\n",
    "print(results[\"result\"])\n",
    "print(\"*\" * 79)\n",
    "for doc in results[\"source_documents\"]:\n",
    "    print(\"-\" * 79)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpO6BRe2uaG9"
   },
   "source": [
    "### `RetrievalQAWithSourcesChain`\n",
    "\n",
    "This variant returns an answer to the question alongside the source documents that were used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5CTjc42uUdP"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "retrieval_qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_qa_with_sources({\"question\": search_query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2ui0Qx9urZB"
   },
   "source": [
    "### [`ConversationalRetrievalChain`](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db)\n",
    "\n",
    "`ConversationalRetrievalChain` remembers and uses previous questions so you can have a chat-like discovery process.\n",
    "\n",
    "To use this chain we must provide a memory class to store and pass the previous messages to the LLM as context. Here we use the `ConversationBufferMemory` class that comes with LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "YIrhXCHZuqby",
    "outputId": "a4fd6924-21e0-481b-f446-4a36890dee2f"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversational_retrieval = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=retriever, memory=memory\n",
    ")\n",
    "\n",
    "search_query = \"Does the Cymbal Starlight have roadside assistance?\"\n",
    "\n",
    "conversational_retrieval.invoke(search_query)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YnHseLqu8g7",
    "outputId": "51baf8d2-c658-474c-86f8-7cd0229d3fa1"
   },
   "outputs": [],
   "source": [
    "new_query = \"What number do I call?\"\n",
    "result = conversational_retrieval.invoke(new_query)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cATTARKvYa7",
    "outputId": "c36d9f48-4276-4f08-e245-df8be40bea62"
   },
   "outputs": [],
   "source": [
    "new_query = \"What information do I need to provide for roadside assistance?\"\n",
    "result = conversational_retrieval.invoke(new_query)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXxBKuMSxgaq"
   },
   "source": [
    "## Advanced: Modifying the default langchain prompt\n",
    "\n",
    "In all of the previous examples we used the default prompt that comes with langchain.\n",
    "\n",
    "We can inspect our chain object to discover the wording of the prompt template being used.\n",
    "\n",
    "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5oU3Wojxf0o",
    "outputId": "5ed0d2ee-88b2-4a6a-90ac-4cdc1be7755f"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "print(qa.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7gBTojOxoAo"
   },
   "source": [
    "---\n",
    "\n",
    "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
    "\n",
    "We create a new prompt_template and pass this in using the `template` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6TfBlI8xl3Y"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"SYSTEM: You are an intelligent assistant helping the users with their questions on their car manual.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
    "\n",
    "Do not try to make up an answer:\n",
    " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
    " - If the context is empty, just say \"I do not know the answer to that.\"\n",
    "\n",
    "=============\n",
    "{context}\n",
    "=============\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNujxYeZySYR"
   },
   "source": [
    "We can also customize the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdsyLnPbyO8c"
   },
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "NUMBER_OF_RESULTS = 10\n",
    "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
    "\n",
    "# Expose index to the retriever\n",
    "retriever = bq_vector_cars_manual.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": NUMBER_OF_RESULTS,\n",
    "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NTWoLozyqBH"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Z4j20ckzOL3",
    "outputId": "08902592-4c67-4891-9ff0-cbfb842f00a0"
   },
   "outputs": [],
   "source": [
    "print(qa.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c5sPXAPzQqa"
   },
   "outputs": [],
   "source": [
    "# Enable for troubleshooting\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.combine_documents_chain.llm_chain.verbose = True\n",
    "qa.combine_documents_chain.llm_chain.llm.verbose = True\n",
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def formatter(result):\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(\".\" * 80)\n",
    "    if \"source_documents\" in result.keys():\n",
    "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"REFERENCE #{idx}\")\n",
    "            print(\"-\" * 80)\n",
    "            if \"score\" in ref.metadata:\n",
    "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
    "            if \"source\" in ref.metadata:\n",
    "                print(f\"Document Source: {ref.metadata['source']}\")\n",
    "            if \"document_name\" in ref.metadata:\n",
    "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
    "            print(\".\" * 80)\n",
    "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
    "    print(\".\" * 80)\n",
    "    print(f\"Response: {wrap(result['result'])}\")\n",
    "    print(\".\" * 80)\n",
    "\n",
    "\n",
    "def wrap(s):\n",
    "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
    "\n",
    "\n",
    "def ask(query, qa=qa, k=NUMBER_OF_RESULTS, search_distance=SEARCH_DISTANCE_THRESHOLD):\n",
    "    qa.retriever.search_kwargs[\"search_distance\"] = search_distance\n",
    "    qa.retriever.search_kwargs[\"k\"] = k\n",
    "    result = qa({\"query\": query})\n",
    "    return formatter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hS6GOAOwzVFC",
    "outputId": "af3b0972-e8df-4cf6-eb93-b47d5949da16"
   },
   "outputs": [],
   "source": [
    "ask(\"What is the capacity of my fuel tank?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
